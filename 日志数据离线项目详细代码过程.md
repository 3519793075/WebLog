# 大数据项目——网站日志数据离线





访问a.html、b.html、c.html产生日志数于目录/home/hadoop/nginx/logs/access_*.log

# 一、使用shell脚本拆解日志文件access.log：

```
#!/bin/sh
logs_path="/home/hadoop/nginx/logs/"
pid_path="/home/hadoop/nginx/logs/nginx.pid"
filepath=${logs_path}"access.log"
echo $filepath
mv ${logs_path}access.log ${logs_path}access_$(date -d '-1 day' '+%Y-%m-%d-%H-%M').log
kill -USR1 `cat ${pid_path}`

```

使用定时器crontab定时执行shell拆解脚本：

```
*/1 * * * * sh /home/hadoop/bigdatasoftware/project1/nginx_log.sh
```



# 二、使用flume采集目录

/home/hadoop/nginx/logs/access_*.log的日志文件到hdfs：hdfs://hadoop-001:8020/weblog/flume-collection/%Y-%m-%d/%H-%M_%{hostname}

```
agent1.sources = source1
agent1.sinks = sink1
agent1.channels = channel1
#监控一个目录下的多个文件新增的内容
agent1.sources.source1.type = TAILDIR
#通过 json 格式存下每个文件消费的偏移量，避免从头消费
agent1.sources.source1.positionFile = /home/hadoop/taildir_position.json
agent1.sources.source1.filegroups = f1
agent1.sources.source1.filegroups.f1 = /home/hadoop/nginx/logs/access_*.log

agent1.sources.source1.interceptors = i1
agent1.sources.source1.interceptors.i1.type = host
agent1.sources.source1.interceptors.i1.hostHeader = hostname
#配置sink组件为hdfs
agent1.sinks.sink1.type = hdfs
agent1.sinks.sink1.hdfs.path=hdfs://localhost:8020/weblog/flume-collection/%Y-%m-%d/%H-%M_%{hostname}
#指定文件名前缀
agent1.sinks.sink1.hdfs.filePrefix = access_log
#指定每批下沉数据的记录条数
agent1.sinks.sink1.hdfs.batchSize= 100
agent1.sinks.sink1.hdfs.fileType = DataStream
agent1.sinks.sink1.hdfs.writeFormat =Text
#指定下沉文件按1MB大小滚动
agent1.sinks.sink1.hdfs.rollSize = 1048576
#指定下沉文件按1000000条数滚动
agent1.sinks.sink1.hdfs.rollCount = 1000000
#指定下沉文件按30分钟滚动
agent1.sinks.sink1.hdfs.rollInterval = 30
#agent1.sinks.sink1.hdfs.round = true
#agent1.sinks.sink1.hdfs.roundValue = 10
#agent1.sinks.sink1.hdfs.roundUnit = minute
agent1.sinks.sink1.hdfs.useLocalTimeStamp = true

#使用memory类型channel
agent1.channels.channel1.type = memory
agent1.channels.channel1.capacity = 500000
agent1.channels.channel1.transactionCapacity = 600

# Bind the source and sink to the channel
agent1.sources.source1.channels = channel1
agent1.sinks.sink1.channel = channel1

```

启动

```
bin/flume-ng agent --conf conf/ --name agent1 --conf-file job/TaildirSource-hdfs.conf -Dflume.root.logger=INFO,console
```



使用shell脚本把文件从：/weblog/flume-collection到另一个目录：log_pre_input=/data/weblog/preprocess/input

shell脚本：movetopreworkdir.sh

```
#!/bin/bash

#
# ===========================================================================
# 程序名称:     
# 功能描述:     移动文件到预处理工作目录
# 输入参数:     运行日期
# 目标路径:     /data/weblog/preprocess/input
# 数据源  :     flume采集数据所存放的路径： /weblog/flume-collection
# 代码审核:     
# 修改人名:
# 修改日期:
# 修改原因:
# 修改列表: 
# ===========================================================================

#flume采集生成的日志文件存放的目录
log_flume_dir=/weblog/flume-collection

#预处理程序的工作目录
log_pre_input=/data/weblog/preprocess/input


#获取时间信息
day_01="2013-09-18"
day_01=`date -d'-1 day' +%Y-%m-%d`
syear=`date --date=$day_01 +%Y`
smonth=`date --date=$day_01 +%m`
sday=`date --date=$day_01 +%d`


#读取日志文件的目录，判断是否有需要上传的文件
files=`hadoop fs -ls $log_flume_dir | grep $day_01 | wc -l`
if [ $files -gt 0 ]; then
hadoop fs -mv ${log_flume_dir}/${day_01} ${log_pre_input}
echo "success moved ${log_flume_dir}/${day_01} to ${log_pre_input} ....."
fi

```

运行脚本文件，进行转移：

sh movetopreworkdir.sh

# **三、上传mp程序jar包到目录：/mr下并依次执行**：



```
hadoop jar etlmr-1.0-SNAPSHOT.jar  com.gec.mr.pre.driver.WeblogEtlPreProcessDriver /data/weblog/preprocess/input/2022-06-05 /data/weblog/preprocess/weblogPreOut/2022-06-05
```



# 四、在hive建表：

`原始数据表：对应mr清洗完之后的数据，而不是原始日志数据`

`drop table if  exists ods_weblog_origin0605;`
create table ods_weblog_origin0610(
valid string,
remote_addr string,
remote_user string,
time_local string,
request string,
status string,
body_bytes_sent string,
http_referer string,
http_user_agent string)
partitioned by (datestr string)
row format delimited
fields terminated by '\001';

将数据导入hive表

~~~ 
load data inpath '/data/weblog/preprocess/weblogPreOut/2022-06-10/' overwrite into table ods_weblog_origin0605 partition(datestr='2022-06-10');
~~~

建表——明细宽表 ods_weblog_detail0610

drop table ods_weblog_detail;
create table ods_weblog_detail0610(
valid           string, --有效标识
remote_addr     string, --来源IP
remote_user     string, --用户标识
time_local      string, --访问完整时间
daystr          string, --访问日期
timestr         string, --访问时间
month           string, --访问月
day             string, --访问日
hour            string, --访问时
request         string, --请求的url
status          string, --响应码
body_bytes_sent string, --传输字节数
http_referer    string, --来源url
ref_host        string, --来源的host
ref_path        string, --来源的路径
ref_query       string, --来源参数query
ref_query_id    string, --来源参数query的值
http_user_agent string --客户终端标识
)
partitioned by(datestr string);





